# AL Development Profile v3.0.0 - Specialist Teammate Roster

## Architecture Shift: Sequential Subagents → Lead-as-Manager with Agent Teams

### v2.x Model (Sequential Subagents)
```
User → Main Session (implements code) → Spawns sequential subagents → Reports back
```
**Problem:** Main session acts as developer, user reviews everything

### v3.0 Model (Lead-as-Manager)
```
User → Lead Session (manages) → Spawns parallel teammate teams → Reviews before presenting to user
```
**Solution:** Lead acts as engineering manager, reviews teammate work, user approves milestones only

---

## Specialist Teammate Roster (12 specialists)

### Planning/Requirements Specialists (2)

#### 1. interview.md (Keep, simplify)
- **Role:** Requirements gathering through structured interviews
- **Spawn:** Single teammate for `/interview` command
- **Input:** User's initial request
- **Output:** Structured interview findings
- **Lead's job:** Review completeness, identify gaps, ask follow-ups

#### 2. solution-architect.md (Rename from solution-planner.md)
- **Role:** Design complete solution architectures for AL/BC
- **Spawn:** 2-3 teammates for competitive design evaluation
- **Input:** Requirements, project context
- **Output:** Complete solution design (tables, pages, codeunits, integration)
- **Lead's job:** Have architects debate trade-offs, pick winning approach

---

### Development Specialist (1)

#### 3. al-developer.md (Keep, simplify for parallel work)
- **Role:** Implement AL code (tables, pages, page extensions, codeunits)
- **Spawn:** N teammates for parallel module development
- **Input:** Solution plan, assigned module/object
- **Output:** AL source code for assigned objects
- **Lead's job:** Ensure consistency across modules, manage file ownership to avoid conflicts

**Key change:** Simplify for focused implementation, remove review/planning responsibilities

---

### Review Specialists (4 - Split from code-reviewer.md)

#### 4. security-reviewer.md (NEW)
- **Role:** Security, permissions, data access patterns review
- **Focus:**
  - Inappropriate permissions
  - Data exposure risks
  - Authentication/authorization issues
  - SetPermission usage

#### 5. al-expert-reviewer.md (NEW)
- **Role:** AL patterns, naming conventions, BC best practices
- **Focus:**
  - Naming conventions
  - Object design patterns
  - Event usage
  - Extension patterns
  - Code organization

#### 6. performance-reviewer.md (NEW)
- **Role:** Database queries, algorithms, resource usage
- **Focus:**
  - N+1 query patterns
  - Inefficient loops
  - Database locking
  - SetLoadFields usage
  - Record variable scope

#### 7. test-coverage-reviewer.md (NEW)
- **Role:** Test adequacy, missing test scenarios
- **Focus:**
  - Untested code paths
  - Missing edge cases
  - Insufficient assertions
  - Test quality

**Lead spawns all 4 for parallel review, has them debate findings, synthesizes to .dev/03-code-review.md**

---

### Test Development Specialists (4 - Split from test-engineer.md)

#### 8. unit-test-engineer.md (NEW)
- **Role:** Individual function/method tests
- **Focus:**
  - Pure functions
  - Calculation logic
  - Validation methods
  - Single codeunit tests
- **Output:** Unit test codeunits

#### 9. integration-test-engineer.md (NEW)
- **Role:** Cross-object interaction tests
- **Focus:**
  - Table → Codeunit interactions
  - Event subscriber tests
  - Multi-object workflows
- **Output:** Integration test codeunits

#### 10. scenario-test-engineer.md (NEW)
- **Role:** End-to-end business scenario tests
- **Focus:**
  - Complete user workflows
  - UI → Business Logic → Database
  - Real-world scenarios
- **Output:** Scenario test codeunits

#### 11. edge-case-test-engineer.md (NEW)
- **Role:** Negative cases, boundaries, error handling
- **Focus:**
  - Boundary values
  - Invalid inputs
  - Error conditions
  - Exception handling
- **Output:** Edge case test codeunits

**Lead spawns all 4 for parallel test development, each owns different test codeunits (no file conflicts)**

---

### Documentation Specialist (1)

#### 12. docs-writer.md (Keep, simplify)
- **Role:** Write comprehensive technical documentation
- **Spawn:** Single teammate (simpler task, no parallelization needed)
- **Input:** Implemented code, test results
- **Output:** Markdown documentation
- **Lead's job:** Review for completeness, technical accuracy

---

## Commands → Specialist Teams Mapping

### /interview
```
Lead → Spawns interview teammate
     → Reviews interview findings
     → Identifies gaps, asks follow-ups
     → Synthesizes requirements
     → Presents to user for approval
```

### /plan
```
Lead → Spawns 2-3 solution-architect teammates
     → Each designs different approach
     → Lead has them debate trade-offs
     → Lead picks winning approach (or hybrid)
     → Writes .dev/02-solution-plan.md
     → Presents to user for approval
```

### /develop
```
Lead → Reads solution plan
     → Identifies parallel modules
     → Spawns N al-developer teammates (1 per module)
     → Each owns different files
     → Lead monitors consistency
     → When complete, spawns review team (4 reviewers)
     → Reviewers debate findings
     → Lead has developers fix critical issues
     → Synthesizes .dev/03-code-review.md
     → Presents to user for approval
```

### /fix (Lightweight, no approval gates)
```
Lead → Quick analysis (1-2 min)
     → If trivial: spawn single al-developer, fix directly
     → If non-trivial: spawn solution-architect for quick plan
     → Spawn al-developer to implement
     → Run compilation check
     → Present fixed code to user (no formal approval gate)
```

### /test
```
Lead → Reads implementation
     → Identifies test scenarios needed
     → Spawns 4 test engineer teammates:
        - Unit tests
        - Integration tests
        - Scenario tests
        - Edge cases
     → Each owns different test codeunits
     → Lead runs bc-test on all codeunits
     → Lead has teammates fix failing tests
     → Synthesizes .dev/05-test-plan.md
     → Presents passing test suite to user
```

### /document
```
Lead → Spawns single docs-writer teammate
     → Reviews documentation for completeness
     → Presents to user
```

---

## File Ownership Strategy (Prevent Conflicts)

### Parallel Development (/develop)
```
Module A (Customer Extensions) → al-developer teammate 1
  Owns: CustomerExt.Table.al, CustomerCardExt.Page.al

Module B (Sales Processing) → al-developer teammate 2
  Owns: SalesProcessor.Codeunit.al, SalesEvents.Codeunit.al

Module C (API) → al-developer teammate 3
  Owns: CustomerAPI.Page.al
```

**Lead's responsibility:** Partition work so teammates don't touch same files

### Parallel Testing (/test)
```
Unit Tests → unit-test-engineer
  Owns: UnitTests.Codeunit.al (ID range 50100-50199)

Integration Tests → integration-test-engineer
  Owns: IntegrationTests.Codeunit.al (ID range 50200-50299)

Scenario Tests → scenario-test-engineer
  Owns: ScenarioTests.Codeunit.al (ID range 50300-50399)

Edge Case Tests → edge-case-test-engineer
  Owns: EdgeCaseTests.Codeunit.al (ID range 50400-50499)
```

**Natural separation:** Different test codeunits = no conflicts

### Parallel Review (/develop, after implementation)
```
Security Reviewer → Reads all code, no writes
AL Expert Reviewer → Reads all code, no writes
Performance Reviewer → Reads all code, no writes
Test Coverage Reviewer → Reads all code, no writes

All write to: .dev/03-code-review.md (lead synthesizes, not teammates)
```

**No conflicts:** Reviewers only read, lead writes synthesis

---

## Lead Session Responsibilities

### 1. Never Implement Code Directly
- Lead does NOT write AL code
- Lead spawns al-developer teammates for all implementation
- Use Shift+Tab for delegate mode to enforce this

### 2. Active Review (Not Rubber-Stamping)
- ❌ Teammate finishes → pass output to user
- ✅ Teammate finishes → review → challenge → refine → present to user

### 3. Quality Gates
Before presenting teammate work to user, verify:
- Consistency across teammates' outputs
- Completeness (no gaps or missing pieces)
- Quality (meets AL coding guidelines)
- Alignment with user's original intent

### 4. Tactical Decisions (Lead Decides)
- Which solution approach to use (after architects debate)
- Whether critical issues are truly fixed
- If test coverage is adequate
- Minor code quality improvements

### 5. Strategic Escalations (Ask User)
- Requirements ambiguities teammates can't resolve
- Architecture decisions with business implications
- Approval gates (after plan, after implementation, after testing)
- Conflicts between teammates lead can't arbitrate

---

## Agent Teams Configuration

Enable in `.claude-plugin/settings.json`:
```json
{
  "env": {
    "CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS": "1"
  },
  "permissions": {
    "allow": []
  },
  "teammateMode": "auto"
}
```

**Teammate modes:**
- `"auto"` - Split panes if in tmux, otherwise in-process
- `"in-process"` - All teammates in main terminal (Shift+Up/Down to switch)
- `"tmux"` - Force split panes (requires tmux or iTerm2)

---

## Migration from v2.x → v3.0

### What's Removed
- ❌ Sequential subagent workflow
- ❌ User reviews every detail
- ❌ 10 unused commands (dev-cycle, estimate, diagnostics, bc-expert, docs-lookup, nav-baseapp, status, and 3 others)
- ❌ 7 unused agents (requirements-engineer, diagnostics-fixer, plan-reviewer, test-reviewer, bc-expert, docs-lookup, dependency-navigator)

### What's New
- ✅ Lead-as-manager paradigm (main session manages, doesn't implement)
- ✅ Parallel agent teams (competing designs, parallel development, parallel review, parallel testing)
- ✅ Specialist teammates (12 focused specialists vs generalist agents)
- ✅ First-level QA by lead (user reviews synthesized outputs, not raw work)
- ✅ Lightweight /fix command (quick iteration, no approval gates)

### What's Kept
- ✅ Document-driven workflow (.dev/ directory)
- ✅ Project context optimization (.dev/project-context.md)
- ✅ Approval gates (after plan, after dev, after test)
- ✅ MCP integration (BC Intelligence, MS Docs, AL Dependency)
- ✅ /init-context command (still valuable for context sharing)

---

## Success Criteria for v3.0

1. **Lead never implements** - All code written by al-developer teammates
2. **User reviews less** - Only synthesized outputs at approval gates
3. **Faster iteration** - Parallel work reduces wall-clock time
4. **Higher quality** - Competing designs + 4 parallel reviewers find more issues
5. **Clear commands** - 7 commands, each with obvious purpose
6. **Lean profile** - Reduced footprint, focused specialists

